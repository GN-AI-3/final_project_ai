from langchain.tools import tool
from tavily import TavilyClient
import os
import psycopg2
import re
from dotenv import load_dotenv
from psycopg2 import sql
import json
from elasticsearch import Elasticsearch
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
from qdrant_client.models import SearchParams

load_dotenv()

elasticsearch_host = os.getenv("ELASTICSEARCH_HOST")
elasticsearch_token = os.getenv("ELASTICSEARCH_SERVICEACCOUNTTOKEN")

es = Elasticsearch(
    elasticsearch_host,
    bearer_auth=elasticsearch_token
).options(ignore_status=400)

exercise_index_name = "exercises"

qdrant_client = QdrantClient(
    url="https://9429a5d7-55d9-43fa-8ad7-8e6cfcd37e22.europe-west3-0.gcp.cloud.qdrant.io:6333", 
    api_key=os.getenv("QDRANT_API_KEY")
)

model = SentenceTransformer('all-mpnet-base-v2')

DB_CONFIG = {
    "dbname": os.getenv("DB_DB"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "host": os.getenv("DB_HOST"),
    "port": os.getenv("DB_PORT")
}

TABLE_SCHEMA = {
    "exercise_record": {
        "columns": ["id", "member_id", "exercise_id", "date", "record_data", "memo_data"],
        "foreign_keys": {
            "member_id": "member.id",
            "exercise_id": "exercise.id"
        },
        "description": "ì‚¬ìš©ìì˜ ê°œë³„ ìš´ë™ ìˆ˜í–‰ ê¸°ë¡. record_dataëŠ” ì„¸íŠ¸/ë°˜ë³µ/ë¬´ê²Œ ë“±ì˜ ìƒì„¸ ê¸°ë¡ì´ë©°, memo_dataëŠ” ììœ  ë©”ëª¨ì…ë‹ˆë‹¤. exercise_idëŠ” exercise í…Œì´ë¸”ì˜ idì™€ ì—°ê²°í•´ ìš´ë™ ì´ë¦„(name)ì„ ì¡°íšŒí•´ì•¼ í•©ë‹ˆë‹¤."
    },
    "member": {
        "columns": ["id", "name", "email", "phone", "profile_image", "goal"],
        "description": "ì‚¬ìš©ì ì •ë³´. goalì€ ì‚¬ìš©ìì˜ ìš´ë™ ëª©í‘œì…ë‹ˆë‹¤ (ì˜ˆ: ë²Œí¬ì—…, ì²´ì¤‘ ê°ëŸ‰)."
    },
    "pt_contract": {
        "columns": ["id","member_id", "trainer_id"],
        "foreign_keys": {
            "member_id": "member.id",
            "trainer_id": "trainer.id"
        },
        "description": "PT ê³„ì•½ ì •ë³´, ì´ í…Œì´ë¸”ì—ì„œ íŠ¸ë ˆì´ë„ˆì˜ íšŒì› ì •ë³´ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆë‹¤."
    },
    "pt_log": {
        "columns": ["id", "member_id", "trainer_id"],
        "foreign_keys": {
            "member_id": "member.id",
            "trainer_id": "trainer.id"
        },
        "description": "PT ìˆ˜ì—… ì¼ì§€"
    },
    "pt_log_exercise": {
        "columns": ["id", "pt_log_id", "exercise_id", "sets", "reps", "weight"],
        "foreign_keys": {
            "pt_log_id": "pt_log.id",
            "exercise_id": "exercise.id"
        },
        "description": "PT ìˆ˜ì—… ì¼ì§€ì— í¬í•¨ëœ ìš´ë™ ì •ë³´"
    }
}

def web_search(query: str) -> str:
    """ì›¹ ê²€ìƒ‰ ìš´ë™ ë£¨í‹´ ì¶”ì²œ"""
    tavily_client = TavilyClient(
        api_key=os.getenv("TAVILY_API_KEY")
    )
    results = tavily_client.search(query)

    filtered_results = sorted(
        [r for r in results.get("results", []) if r.get("score", 0) >= 0.7],
        key=lambda x: x.get("score", 0),
        reverse=True
    )[:3]

    return json.dumps(filtered_results, indent=2, ensure_ascii=False)

@tool
def get_user_goal(user_id: str) -> str:
    """PostgreSQL - member tableì—ì„œ ì‚¬ìš©ì ëª©í‘œ ì •ë³´ ì¡°íšŒ"""
    query = f"SELECT goal FROM member WHERE id = '{user_id}';"
    print("query: ", query)
    try:
        with psycopg2.connect(**DB_CONFIG) as conn:
            with conn.cursor() as cursor:
                cursor.execute(query)
                result = cursor.fetchall()
                return str(result)
    except Exception as e:
        return f"Database error: {str(e)}"
    
@tool
def get_user_physical_info(user_id: str) -> str:
    """PostgreSQL - inbody tableì—ì„œ ì‚¬ìš©ì ì‹ ì²´ ì •ë³´ ì¡°íšŒ"""
    query = f"SELECT tall, weight, bmi FROM inbody WHERE member_id = {user_id};"
    print("query: ", query)
    try:
        with psycopg2.connect(**DB_CONFIG) as conn:
            with conn.cursor() as cursor:
                cursor.execute(query)
                result = cursor.fetchall()

        if result:
            row = result[0]
            tall, weight, bmi = row[0], row[1], row[2]
            return (
                f"ì‚¬ìš©ì ì‹ ì²´ ì •ë³´:\n"
                f"- í‚¤: {tall}cm\n"
                f"- ëª¸ë¬´ê²Œ: {weight}kg\n"
                f"- BMI: {bmi}\n"
            )
        else:
            return "ì‚¬ìš©ìì˜ ì‹ ì²´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"
    except Exception as e:
        return f"Database error: {str(e)}"

@tool
def get_user_exercise_record(user_id: str) -> str:
    """PostgreSQL - exercise_record tableì—ì„œ ì‚¬ìš©ì ìš´ë™ ê¸°ë¡ ì¡°íšŒ"""
    query = f"SELECT * FROM exercise_record WHERE member_id = {user_id};"
    print("query: ", query)
    try:
        with psycopg2.connect(**DB_CONFIG) as conn:
            with conn.cursor() as cursor:
                cursor.execute(query)
                result = cursor.fetchall()
                return str(result)
    except Exception as e:
        return f"Database error: {str(e)}"

def get_all_table_schema() -> str:
    """PostgreSQL - ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ ì •ë³´ë¥¼ ë°˜í™˜"""
    schema_info = []
    for table_name, table_info in TABLE_SCHEMA.items():
        schema_info.append(f"í…Œì´ë¸”: {table_name}")
        schema_info.append(f"ì»¬ëŸ¼: {', '.join(table_info['columns'])}")
        schema_info.append(f"ì„¤ëª…: {table_info['description']}")
        schema_info.append("--------------------------------")
    return "\n".join(schema_info)

def master_select_db(table_name: str, column_name: str, value: str) -> str:
    """PostgreSQL - ì‚¬ì „ ì •ì˜ëœ ëª¨ë“  í…Œì´ë¸”ì—ì„œ í…Œì´ë¸”ëª…, ì»¬ëŸ¼ëª…, ê°’ìœ¼ë¡œ ë°ì´í„° ì¡°íšŒ
    table_name, column_name, value ëª¨ë‘ í•„ìˆ˜ì…ë‹ˆë‹¤.
    TABLE_SCHEMA ì— ì •ì˜ëœ í…Œì´ë¸”ë§Œ ì‚¬ìš© ê°€ëŠ¥
    TABLE_SCHEMA ì— ì •ì˜ëœ ì»¬ëŸ¼ë§Œ ì‚¬ìš© ê°€ëŠ¥
    
    Args:
        table_name: ì¡°íšŒí•  í…Œì´ë¸” ì´ë¦„
        column_name: (ì„ íƒ) ì¡°ê±´ ì»¬ëŸ¼ ì´ë¦„
        value: (ì„ íƒ) ì¡°ê±´ ê°’
        
    Returns:
        ì¡°íšŒëœ ë°ì´í„°ì˜ JSON ë¬¸ìì—´
    """
    if table_name not in TABLE_SCHEMA:
        print("table_name: ", table_name)
        return "Invalid table name"
    
    if column_name not in TABLE_SCHEMA[table_name]["columns"]:
        print("column_name: ", column_name)
        return "Invalid column name"
    
    try:
        query = sql.SQL("SELECT * FROM {} WHERE {} = %s").format(
            sql.Identifier(table_name),
            sql.Identifier(column_name)
        )

        params = (value,)
        print("query: ", query)
        print("params: ", params)

        with psycopg2.connect(**DB_CONFIG) as conn:
            with conn.cursor() as cursor:
                cursor.execute(query, params)
                rows = cursor.fetchall()
                column_names = [desc[0] for desc in cursor.description]

                result = [dict(zip(column_names, row)) for row in rows]
                return json.dumps(result, indent=2, ensure_ascii=False, default=str)
    except Exception as e:
        return f"Database error: {str(e)}"

def master_select_db_multi(
    table_name: str, 
    conditions: dict
) -> str:
    """
    PostgreSQL - ì‚¬ì „ ì •ì˜ëœ í…Œì´ë¸”ì—ì„œ ì—¬ëŸ¬ ì¡°ê±´(column=value)ìœ¼ë¡œ ë°ì´í„° ì¡°íšŒ
    TABLE_SCHEMA ì— ì •ì˜ëœ í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ë§Œ ì‚¬ìš© ê°€ëŠ¥

    Args:
        table_name: ì¡°íšŒí•  í…Œì´ë¸” ì´ë¦„
        conditions: ì¡°íšŒ ì¡°ê±´ (ì˜ˆ: {"id": "1", "email": "test@gmail.com"})

    Returns:
        JSON ë¬¸ìì—´ë¡œ ëœ ê²°ê³¼
    """
    if table_name not in TABLE_SCHEMA:
        return "Invalid table name"
    
    for col in conditions.keys():
        if col not in TABLE_SCHEMA[table_name]["columns"]:
            return f"Invalid column name: {col}"

    try:
        where_clauses = [
            sql.SQL("{} = %s").format(sql.Identifier(col)) for col in conditions.keys()
        ]
        query = sql.SQL("SELECT * FROM {} WHERE {}").format(
            sql.Identifier(table_name),
            sql.SQL(" AND ").join(where_clauses)
        )

        params = tuple(conditions.values())

        with psycopg2.connect(**DB_CONFIG) as conn:
            with conn.cursor() as cursor:
                cursor.execute(query, params)
                rows = cursor.fetchall()
                column_names = [desc[0] for desc in cursor.description]

                result = [dict(zip(column_names, row)) for row in rows]
                return json.dumps(result, indent=2, ensure_ascii=False, default=str)
    except Exception as e:
        return json.dumps({"error": f"Database error: {str(e)}"})
    
# def master_select_db_multi(table_name: str, conditions: dict) -> str:
#     """
#     PostgreSQL - ì§€ì •ëœ í…Œì´ë¸”ì— ëŒ€í•´ ì¡°ê±´ ê¸°ë°˜ ì¡°íšŒ ìˆ˜í–‰.
#     ì¡°ê±´ ì»¬ëŸ¼ì´ ë‹¤ë¥¸ í…Œì´ë¸”ì— ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ JOIN ìˆ˜í–‰.

#     Args:
#         table_name (str): ê¸°ì¤€ í…Œì´ë¸”
#         conditions (dict): ì¡°ê±´ ì»¬ëŸ¼ê³¼ ê°’ (ì˜ˆ: {"name": "ì¥ê·¼ìš°"})

#     Returns:
#         str: JSON ê²°ê³¼ ë˜ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€
#     """
#     if table_name not in TABLE_SCHEMA:
#         return json.dumps({"error": "Invalid table name"})

#     try:
#         base_alias = "t0"
#         joins = []
#         where_clauses = []
#         used_tables = {table_name: base_alias}
#         alias_counter = 1

#         for cond_col, cond_val in conditions.items():
#             found = False
#             for t_name, schema in TABLE_SCHEMA.items():
#                 if cond_col in schema["columns"]:
#                     found = True
#                     if t_name not in used_tables:
#                         alias = f"t{alias_counter}"
#                         alias_counter += 1
#                         used_tables[t_name] = alias

#                         # ì¡°ì¸ ê²½ë¡œ ì¶”ë¡ 
#                         for fk_col, ref in TABLE_SCHEMA[t_name].get("foreign_keys", {}).items():
#                             ref_table, ref_col = ref.split(".")
#                             if ref_table in used_tables:
#                                 joins.append(
#                                     sql.SQL("JOIN {} {} ON {}.{} = {}.{}").format(
#                                         sql.Identifier(t_name),
#                                         sql.Identifier(alias),
#                                         sql.Identifier(alias),
#                                         sql.Identifier(fk_col),
#                                         sql.Identifier(used_tables[ref_table]),
#                                         sql.Identifier(ref_col),
#                                     )
#                                 )

#                     where_clauses.append(
#                         sql.SQL("{}.{} = %s").format(
#                             sql.Identifier(used_tables[t_name]),
#                             sql.Identifier(cond_col)
#                         )
#                     )
#                     break
#             if not found:
#                 return json.dumps({"error": f"Invalid column name: {cond_col}"})

#         select_fields = sql.SQL(", ").join([
#             sql.SQL(f"{alias}.*") for alias in used_tables.values()
#         ])

#         from_clause = sql.SQL("{} {}").format(
#             sql.Identifier(table_name),
#             sql.SQL("AS {}").format(sql.Identifier(base_alias))
#         )

#         query = sql.SQL("SELECT {} FROM {} {} WHERE {}").format(
#             select_fields,
#             from_clause,
#             sql.SQL(" ").join(joins),
#             sql.SQL(" AND ").join(where_clauses)
#         )

#         params = list(conditions.values())

#         with psycopg2.connect(**DB_CONFIG) as conn:
#             with conn.cursor() as cursor:
#                 cursor.execute(query, params)
#                 rows = cursor.fetchall()
#                 column_names = [desc[0] for desc in cursor.description]

#                 result = [dict(zip(column_names, row)) for row in rows]
#                 return json.dumps(result, indent=2, ensure_ascii=False, default=str)

#     except Exception as e:
#         return json.dumps({"error": f"Database error: {str(e)}"})

def search_exercise_by_name(name: str):
    name_compact = name.replace(" ", "")

    try:
        res = es.search(
            index=exercise_index_name,
            query={
                "bool": {
                    "should": [
                        {
                            "term": {
                                "name_compact": {
                                    "value": name_compact,
                                    "boost": 30  # ì •í™• ì¼ì¹˜ ìµœê³  ê°€ì¤‘ì¹˜
                                }
                            }
                        },
                        {
                            "match_phrase": {
                                "name": {
                                    "query": name,
                                    "boost": 10
                                }
                            }
                        },
                        {
                            "multi_match": {
                                "query": name,
                                "type": "best_fields",
                                "fields": ["name^1", "name._2gram^0.2", "name._3gram^0.1"]
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            }
        )

        hits = res["hits"]["hits"]
        if not hits:
            return json.dumps([])

        max_score = hits[0]["_score"]
        threshold = max_score * 0.98

        exercises = [
            {
                "id": hit["_source"]["exercise_id"],
                "name": hit["_source"]["name"]
            }
            for hit in hits if hit["_score"] >= threshold
        ]
        
        if len(exercises) == 1:
            result = exercises[0]  # ë‹¨ì¼ ê°ì²´
        else:
            result = exercises

        return json.dumps(result, indent=2, ensure_ascii=False, default=str)
    except Exception as e:
        return json.dumps({"error": f"Database error: {str(e)}"})
    
def retrieve_exercise_info_by_similarity(query: str):
    """Qdrant - ìš´ë™ ì •ë³´ ê²€ìƒ‰"""
    query_vector = model.encode(query).tolist()
    try:
        res = qdrant_client.search(
            collection_name="exercises",
            query_vector=query_vector,
            limit=3,
            search_params=SearchParams(hnsw_ef=128, exact=False)
        )

        filtered_results = [
            {
                "score": round(result.score, 4),
                "content": result.payload.get("content", "No content")
            }
            for result in res if result.score >= 0.6
        ]

        for i, item in enumerate(filtered_results):
            print(f"{i+1}. ğŸ”¹ Score: {item['score']}\n   ğŸ“„ Content: {item['content']}\n")

        return json.dumps(filtered_results, indent=2, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": f"Database error: {str(e)}"})
